{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"competition","sourceId":63056,"databundleVersionId":9094797}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from fastai.vision.all import *\nset_seed(42)\n\nfrom fastcore.parallel import *\nfrom fastai.data.core import *\nimport torch\nimport time","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-08-21T17:17:07.060242Z","iopub.execute_input":"2024-08-21T17:17:07.060648Z","iopub.status.idle":"2024-08-21T17:17:07.074578Z","shell.execute_reply.started":"2024-08-21T17:17:07.060612Z","shell.execute_reply":"2024-08-21T17:17:07.073544Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\n\ntrain_metadata_path = Path('/kaggle/input/isic-2024-challenge/train-metadata.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:17:07.076569Z","iopub.execute_input":"2024-08-21T17:17:07.076905Z","iopub.status.idle":"2024-08-21T17:17:07.089959Z","shell.execute_reply.started":"2024-08-21T17:17:07.076878Z","shell.execute_reply":"2024-08-21T17:17:07.088902Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"#  Delete all exciting files so there will be no overlaping  \n","metadata":{}},{"cell_type":"code","source":"# Path to the Kaggle working directory\nworking_dir = '/kaggle/working'\n\n# Loop through all files and folders in the directory\nfor filename in os.listdir(working_dir):\n    file_path = os.path.join(working_dir, filename)\n    try:\n        # Check if it's a file or a folder\n        if os.path.isfile(file_path) or os.path.islink(file_path):\n            os.unlink(file_path)  # Delete the file\n        elif os.path.isdir(file_path):\n            shutil.rmtree(file_path)  # Delete the folder and its contents\n    except Exception as e:\n        print(f'Failed to delete {file_path}. Reason: {e}')\n\nprint(\"All files and folders in the working directory have been deleted.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:17:07.091449Z","iopub.execute_input":"2024-08-21T17:17:07.092538Z","iopub.status.idle":"2024-08-21T17:17:07.178644Z","shell.execute_reply.started":"2024-08-21T17:17:07.092499Z","shell.execute_reply":"2024-08-21T17:17:07.177604Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"All files and folders in the working directory have been deleted.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Looked at the sizes of the images to select the optimal one\n# Takes a while so I just commented it \n'''from pathlib import Path\nfrom PIL import Image\nfrom collections import defaultdict\n\ndef get_image_sizes_and_counts(images_path):\n    # Ensure images_path is a Path object\n    images_path = Path(images_path)\n    \n    # Dictionary to store image sizes and their counts\n    size_counts = defaultdict(int)\n    \n    # Iterate over all image files in the directory\n    for img_path in images_path.glob('*'):\n        try:\n            with Image.open(img_path) as img:\n                width, height = img.size\n                size_counts[(width, height)] += 1\n        except Exception as e:\n            print(f\"Error opening {img_path}: {e}\")\n    \n    return size_counts\n\n# Define the path to your images\nimages_path = Path('/kaggle/input/isic-2024-challenge/train-image/image/')\n\n# Get all image sizes and their counts\nsize_counts = get_image_sizes_and_counts(images_path)\n\n # Convert dictionary to list of tuples and sort by count\nsorted_sizes = sorted(size_counts.items(), key=lambda x: x[1], reverse=True)\n\n# Get top 5 sizes\ntop_5_sizes = sorted_sizes[:5]\n\nprint(top_5_sizes)'''","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:17:07.180236Z","iopub.execute_input":"2024-08-21T17:17:07.180633Z","iopub.status.idle":"2024-08-21T17:17:07.189740Z","shell.execute_reply.started":"2024-08-21T17:17:07.180593Z","shell.execute_reply":"2024-08-21T17:17:07.188587Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"'from pathlib import Path\\nfrom PIL import Image\\nfrom collections import defaultdict\\n\\ndef get_image_sizes_and_counts(images_path):\\n    # Ensure images_path is a Path object\\n    images_path = Path(images_path)\\n    \\n    # Dictionary to store image sizes and their counts\\n    size_counts = defaultdict(int)\\n    \\n    # Iterate over all image files in the directory\\n    for img_path in images_path.glob(\\'*\\'):\\n        try:\\n            with Image.open(img_path) as img:\\n                width, height = img.size\\n                size_counts[(width, height)] += 1\\n        except Exception as e:\\n            print(f\"Error opening {img_path}: {e}\")\\n    \\n    return size_counts\\n\\n# Define the path to your images\\nimages_path = Path(\\'/kaggle/input/isic-2024-challenge/train-image/image/\\')\\n\\n# Get all image sizes and their counts\\nsize_counts = get_image_sizes_and_counts(images_path)\\n\\n # Convert dictionary to list of tuples and sort by count\\nsorted_sizes = sorted(size_counts.items(), key=lambda x: x[1], reverse=True)\\n\\n# Get top 5 sizes\\ntop_5_sizes = sorted_sizes[:5]\\n\\nprint(top_5_sizes)'"},"metadata":{}}]},{"cell_type":"markdown","source":"Based on the code aboce top 5 are\n\n\t•   (133, 133): 21,049 images\n\t•\t(131, 131): 20,906 images\n\t•\t(129, 129): 20,379 images\n\t•\t(135, 135): 20,364 images\n\t•\t(137, 137): 18,927 images\n    \nso I took the standard size =137,137","metadata":{}},{"cell_type":"markdown","source":"# Cleaning metadata","metadata":{}},{"cell_type":"code","source":"# Load metadata append\n\ndf = pd.read_csv(train_metadata_path,low_memory=False)\n\n# Drop specified columns\ncolumns_to_drop = ['copyright_license', 'attribution', 'image_type', 'iddx_1', 'iddx_2', 'iddx_3', 'iddx_4',\n                       'iddx_5', 'iddx_full', 'mel_mitotic_index', 'mel_thick_mm', 'tbp_tile_type', \n                       'tbp_lv_dnn_lesion_confidence', 'lesion_id']\n\n# Define categorical and continuous columns\ncat_names = [ 'sex', 'anatom_site_general', 'tbp_lv_location', 'tbp_lv_location_simple']\ncont_names = [x for x in df.columns if x not in (cat_names + ['target', 'isic_id','patient_id']+columns_to_drop)]\ny_col = 'target'\nimage_col = 'isic_id'","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:17:07.192079Z","iopub.execute_input":"2024-08-21T17:17:07.192414Z","iopub.status.idle":"2024-08-21T17:17:16.996609Z","shell.execute_reply.started":"2024-08-21T17:17:07.192388Z","shell.execute_reply":"2024-08-21T17:17:16.995388Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def process_data(df, cat_names):\n    \n    \n    # Drop columns \n    df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n    \n    # Add number of pictures for each patient\n    df['numb_pic'] = df.groupby('patient_id')['patient_id'].transform('count')\n    \n    # Fill missing values with the mode\n    if 'age_approx' in df.columns:\n        mode_age = df['age_approx'].mode()[0]\n        df['age_approx'] = df['age_approx'].fillna(mode_age)\n    \n    if 'sex' in df.columns:\n        mode_sex = df['sex'].mode()[0]\n        df['sex'] = df['sex'].fillna(mode_sex)\n    \n    # Convert categorical columns to dummies\n    df = pd.get_dummies(df, columns=cat_names, prefix=cat_names)\n    \n    # Get new categorical column names\n    new_cat_columns = [col for col in df.columns if any(col.startswith(name + '_') for name in cat_names)]\n    \n    # Ensure 'isic_id' in df has the correct file extension\n    # if 'isic_id' in df.columns:\n    #     df['isic_id'] = df['isic_id'].apply(lambda x: x.strip() + '.jpg')\n    \n    return df, new_cat_columns\n\n#Apply to df\ndf, new_cat_columns= process_data(df,cat_names)  ","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:17:16.997875Z","iopub.execute_input":"2024-08-21T17:17:16.998177Z","iopub.status.idle":"2024-08-21T17:17:17.536866Z","shell.execute_reply.started":"2024-08-21T17:17:16.998151Z","shell.execute_reply":"2024-08-21T17:17:17.535704Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def save_columns_to_csv(columns_list, file_path):\n    df = pd.DataFrame(columns_list, columns=['new_cat_columns'])\n    df.to_csv(file_path, index=False)\n\n# Example usage\nsave_columns_to_csv(new_cat_columns, '/kaggle/working/new_cat_columns.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:17:17.538679Z","iopub.execute_input":"2024-08-21T17:17:17.539444Z","iopub.status.idle":"2024-08-21T17:17:17.547947Z","shell.execute_reply.started":"2024-08-21T17:17:17.539402Z","shell.execute_reply":"2024-08-21T17:17:17.546942Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# Oversampling","metadata":{}},{"cell_type":"code","source":"import h5py\nimport numpy as np\nimport pandas as pd\nimport random\nfrom PIL import Image, ImageEnhance\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom tqdm import tqdm\n\ndef augment_image(image_array, target_size=(137, 137)):\n    # Convert numpy array to PIL image\n    pil_image = Image.fromarray(image_array)\n    \n    # Randomly adjust hue, contrast, rotation, etc.\n    if random.random() > 0.5:\n        enhancer = ImageEnhance.Contrast(pil_image)\n        pil_image = enhancer.enhance(random.uniform(0.8, 1.2))\n    if random.random() > 0.5:\n        enhancer = ImageEnhance.Color(pil_image)\n        pil_image = enhancer.enhance(random.uniform(0.8, 1.2))\n    if random.random() > 0.5:\n        pil_image = pil_image.rotate(random.uniform(-10, 10), resample=Image.Resampling.BICUBIC, expand=True)\n    \n    # Resize the image to the target size without padding\n    pil_image = pil_image.resize(target_size, resample=Image.Resampling.BICUBIC)\n    \n    # Convert PIL image back to numpy array\n    return np.array(pil_image)\n\ndef resize_and_pad_image(image_array, standard_size=(137, 137)):\n    if image_array.size == 0:\n        raise ValueError(\"The image array is empty.\")\n    \n    if len(image_array.shape) not in [2, 3]:\n        raise ValueError(f\"Unexpected image_array shape: {image_array.shape}\")\n\n    if image_array.ndim == 2:  # Grayscale image\n        image_array = np.stack([image_array] * 3, axis=-1)\n    elif image_array.ndim == 3 and image_array.shape[2] == 1:  # Single-channel image\n        image_array = np.concatenate([image_array] * 3, axis=-1)\n    \n    old_size = image_array.shape[:2]  # original size (height, width)\n    new_size = standard_size  # (width, height)\n\n    if not isinstance(new_size, tuple) or len(new_size) != 2:\n        raise ValueError(f\"Expected standard_size to be a tuple of length 2, got {new_size}\")\n\n    if not old_size:\n        raise ValueError(\"The old_size is empty. Cannot resize an image with unknown dimensions.\")\n\n    ratio = float(new_size[0]) / max(old_size)\n    new_image_size = (int(old_size[1] * ratio), int(old_size[0] * ratio))\n\n    if not isinstance(new_image_size, tuple) or len(new_image_size) != 2:\n        raise ValueError(f\"Expected new_image_size to be a tuple of length 2, got {new_image_size}\")\n\n    try:\n        pil_image = Image.fromarray(image_array)\n        resized_image = pil_image.resize(new_image_size, resample=Image.Resampling.LANCZOS)\n        resized_image_array = np.array(resized_image)\n        \n        new_image = np.ones((new_size[1], new_size[0], image_array.shape[2]), dtype=np.uint8) * 255\n        \n        top = (new_size[1] - new_image_size[1]) // 2\n        left = (new_size[0] - new_image_size[0]) // 2\n        \n        new_image[top:top + new_image_size[1], left:left + new_image_size[0]] = resized_image_array\n\n    except Exception as e:\n        print(f\"Failed to process image: {e}\")\n        return np.zeros(standard_size + (image_array.shape[2],), dtype=np.uint8)  # Return a blank image if processing fails\n    \n    return new_image\n\ndef save_image_to_hdf5(hdf5_file, image_array, image_name, group_name):\n    try:\n        # Resize and pad the image\n        image_array = resize_and_pad_image(image_array, standard_size=(137, 137))\n        hdf5_file[group_name].create_dataset(image_name, data=image_array, compression=\"gzip\", compression_opts=9)\n        return image_name\n    except Exception as e:\n        print(f\"Failed to save image {image_name}: {e}\")\n        return None\n\ndef read_image_from_folder(input_folder, img_name):\n    img_path = f\"{input_folder}/{img_name}.jpg\"\n    image = Image.open(img_path)\n    return np.array(image)\n\ndef synthetic_oversample(df, column):\n    # Replace this function with your synthetic oversampling logic\n    return random.choice(df[column].values)\n\ndef oversample_data(input_folder, df, oversample_count=10000, undersample_count=10000, output_hdf5_path='oversampled_images.hdf5', batch_size=100):\n    cancer_df = df[df['target'] == 1]\n    non_cancer_df = df[df['target'] == 0].sample(n=undersample_count, random_state=42)\n\n    # Oversample the minority class\n    oversampled_rows = []\n    cancer_len = len(cancer_df)\n    for i in range(oversample_count - cancer_len):\n        new_row = {}\n        original_row = cancer_df.iloc[i % cancer_len]  # Reuse rows as needed\n        new_row['isic_id'] = f\"{original_row['isic_id']}_oversampled_{(i // cancer_len) + 1}\"\n        new_row['target'] = 1\n        for column in df.columns:\n            if column != 'isic_id' and column != 'target':\n                new_row[column] = synthetic_oversample(cancer_df, column)\n        oversampled_rows.append(new_row)\n    \n    oversampled_df = pd.DataFrame(oversampled_rows)\n    combined_df = pd.concat([non_cancer_df, cancer_df, oversampled_df], ignore_index=True)\n\n    # Save images to HDF5\n    with h5py.File(output_hdf5_path, 'w') as hdf_out:\n        hdf_out.create_group('oversampled_images')\n        futures = []\n        with ThreadPoolExecutor() as executor:\n            # Process non-cancer and original cancer images\n            for i in range(0, len(combined_df), batch_size):\n                batch_files = combined_df['isic_id'].tolist()[i:i + batch_size]\n                for img_name in tqdm(batch_files, desc=f\"Processing batch {i // batch_size + 1}\"):\n                    try:\n                        if '_oversampled_' not in img_name:  # Original images\n                            img_data = read_image_from_folder(input_folder, img_name)\n                        else:  # Augmented images\n                            original_img_name = img_name.split('_oversampled_')[0]\n                            img_data = read_image_from_folder(input_folder, original_img_name)\n                            img_data = augment_image(img_data)\n                        \n                        futures.append(executor.submit(save_image_to_hdf5, hdf_out, img_data, img_name, 'oversampled_images'))\n                    except Exception as e:\n                        print(f\"Failed to process image {img_name}: {e}\")\n                \n                for future in as_completed(futures):\n                    future.result()\n\n    return combined_df","metadata":{"execution":{"iopub.status.busy":"2024-08-21T18:03:44.987976Z","iopub.execute_input":"2024-08-21T18:03:44.988448Z","iopub.status.idle":"2024-08-21T18:03:45.014236Z","shell.execute_reply.started":"2024-08-21T18:03:44.988412Z","shell.execute_reply":"2024-08-21T18:03:45.013210Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"input_folder= Path(\"/kaggle/input/isic-2024-challenge/train-image/image\")\noutput_hdf5_path = Path(\"/kaggle/working/Oversampled_small.hdf5\")\ncombined_df = oversample_data(input_folder, df, undersample_count=100000, oversample_count=10000, output_hdf5_path=output_hdf5_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T18:03:50.183972Z","iopub.execute_input":"2024-08-21T18:03:50.184386Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Processing batch 1: 100%|██████████| 100/100 [00:00<00:00, 624.81it/s]\nProcessing batch 2: 100%|██████████| 100/100 [00:00<00:00, 545.78it/s]\nProcessing batch 3: 100%|██████████| 100/100 [00:00<00:00, 574.96it/s]\nProcessing batch 4: 100%|██████████| 100/100 [00:00<00:00, 648.68it/s]\nProcessing batch 5: 100%|██████████| 100/100 [00:00<00:00, 688.28it/s]\nProcessing batch 6: 100%|██████████| 100/100 [00:00<00:00, 652.38it/s]\nProcessing batch 7: 100%|██████████| 100/100 [00:00<00:00, 662.29it/s]\nProcessing batch 8: 100%|██████████| 100/100 [00:00<00:00, 684.99it/s]\nProcessing batch 9: 100%|██████████| 100/100 [00:00<00:00, 658.07it/s]\nProcessing batch 10: 100%|██████████| 100/100 [00:00<00:00, 656.87it/s]\nProcessing batch 11: 100%|██████████| 100/100 [00:00<00:00, 679.46it/s]\nProcessing batch 12: 100%|██████████| 100/100 [00:00<00:00, 664.83it/s]\nProcessing batch 13: 100%|██████████| 100/100 [00:00<00:00, 648.67it/s]\nProcessing batch 14: 100%|██████████| 100/100 [00:00<00:00, 694.05it/s]\nProcessing batch 15: 100%|██████████| 100/100 [00:00<00:00, 688.20it/s]\nProcessing batch 16: 100%|██████████| 100/100 [00:00<00:00, 654.32it/s]\nProcessing batch 17: 100%|██████████| 100/100 [00:00<00:00, 686.99it/s]\nProcessing batch 18: 100%|██████████| 100/100 [00:00<00:00, 667.11it/s]\nProcessing batch 19: 100%|██████████| 100/100 [00:00<00:00, 685.11it/s]\nProcessing batch 20: 100%|██████████| 100/100 [00:00<00:00, 667.40it/s]\nProcessing batch 21: 100%|██████████| 100/100 [00:00<00:00, 693.68it/s]\nProcessing batch 22: 100%|██████████| 100/100 [00:00<00:00, 678.04it/s]\nProcessing batch 23: 100%|██████████| 100/100 [00:00<00:00, 684.74it/s]\nProcessing batch 24: 100%|██████████| 100/100 [00:00<00:00, 630.99it/s]\nProcessing batch 25: 100%|██████████| 100/100 [00:00<00:00, 635.13it/s]\nProcessing batch 26: 100%|██████████| 100/100 [00:00<00:00, 677.20it/s]\nProcessing batch 27: 100%|██████████| 100/100 [00:00<00:00, 635.77it/s]\nProcessing batch 28: 100%|██████████| 100/100 [00:00<00:00, 667.05it/s]\nProcessing batch 29: 100%|██████████| 100/100 [00:00<00:00, 620.03it/s]\nProcessing batch 30: 100%|██████████| 100/100 [00:00<00:00, 637.15it/s]\nProcessing batch 31: 100%|██████████| 100/100 [00:00<00:00, 605.67it/s]\nProcessing batch 32: 100%|██████████| 100/100 [00:00<00:00, 632.42it/s]\nProcessing batch 33: 100%|██████████| 100/100 [00:00<00:00, 633.75it/s]\nProcessing batch 34: 100%|██████████| 100/100 [00:00<00:00, 599.01it/s]\nProcessing batch 35: 100%|██████████| 100/100 [00:00<00:00, 597.50it/s]\nProcessing batch 36: 100%|██████████| 100/100 [00:00<00:00, 586.99it/s]\nProcessing batch 37: 100%|██████████| 100/100 [00:00<00:00, 526.27it/s]\nProcessing batch 38: 100%|██████████| 100/100 [00:00<00:00, 668.62it/s]\nProcessing batch 39: 100%|██████████| 100/100 [00:00<00:00, 647.02it/s]\nProcessing batch 40: 100%|██████████| 100/100 [00:00<00:00, 563.82it/s]\nProcessing batch 41: 100%|██████████| 100/100 [00:00<00:00, 606.70it/s]\nProcessing batch 42: 100%|██████████| 100/100 [00:00<00:00, 700.35it/s]\nProcessing batch 43: 100%|██████████| 100/100 [00:00<00:00, 612.10it/s]\nProcessing batch 44: 100%|██████████| 100/100 [00:00<00:00, 631.53it/s]\nProcessing batch 45: 100%|██████████| 100/100 [00:00<00:00, 641.05it/s]\nProcessing batch 46: 100%|██████████| 100/100 [00:00<00:00, 675.50it/s]\nProcessing batch 47: 100%|██████████| 100/100 [00:00<00:00, 597.30it/s]\nProcessing batch 48: 100%|██████████| 100/100 [00:00<00:00, 508.36it/s]\nProcessing batch 49: 100%|██████████| 100/100 [00:00<00:00, 526.54it/s]\nProcessing batch 50: 100%|██████████| 100/100 [00:00<00:00, 657.91it/s]\nProcessing batch 51: 100%|██████████| 100/100 [00:00<00:00, 713.47it/s]\nProcessing batch 52: 100%|██████████| 100/100 [00:00<00:00, 662.32it/s]\nProcessing batch 53: 100%|██████████| 100/100 [00:00<00:00, 526.02it/s]\nProcessing batch 54: 100%|██████████| 100/100 [00:00<00:00, 558.81it/s]\nProcessing batch 55: 100%|██████████| 100/100 [00:00<00:00, 654.20it/s]\nProcessing batch 56: 100%|██████████| 100/100 [00:00<00:00, 678.52it/s]\nProcessing batch 57: 100%|██████████| 100/100 [00:00<00:00, 672.67it/s]\nProcessing batch 58: 100%|██████████| 100/100 [00:00<00:00, 649.69it/s]\nProcessing batch 59: 100%|██████████| 100/100 [00:00<00:00, 664.44it/s]\nProcessing batch 60: 100%|██████████| 100/100 [00:00<00:00, 676.40it/s]\nProcessing batch 61: 100%|██████████| 100/100 [00:00<00:00, 590.59it/s]\nProcessing batch 62: 100%|██████████| 100/100 [00:00<00:00, 713.37it/s]\nProcessing batch 63: 100%|██████████| 100/100 [00:00<00:00, 642.32it/s]\nProcessing batch 64: 100%|██████████| 100/100 [00:00<00:00, 623.10it/s]\nProcessing batch 65: 100%|██████████| 100/100 [00:00<00:00, 595.51it/s]\nProcessing batch 66: 100%|██████████| 100/100 [00:00<00:00, 680.26it/s]\nProcessing batch 67: 100%|██████████| 100/100 [00:00<00:00, 708.85it/s]\nProcessing batch 68: 100%|██████████| 100/100 [00:00<00:00, 680.43it/s]\nProcessing batch 69: 100%|██████████| 100/100 [00:00<00:00, 629.26it/s]\nProcessing batch 70: 100%|██████████| 100/100 [00:00<00:00, 650.39it/s]\nProcessing batch 71: 100%|██████████| 100/100 [00:00<00:00, 669.87it/s]\nProcessing batch 72: 100%|██████████| 100/100 [00:00<00:00, 667.14it/s]\nProcessing batch 73: 100%|██████████| 100/100 [00:00<00:00, 701.32it/s]\nProcessing batch 74: 100%|██████████| 100/100 [00:00<00:00, 637.30it/s]\nProcessing batch 75: 100%|██████████| 100/100 [00:00<00:00, 695.27it/s]\nProcessing batch 76: 100%|██████████| 100/100 [00:00<00:00, 638.06it/s]\nProcessing batch 77: 100%|██████████| 100/100 [00:00<00:00, 644.64it/s]\nProcessing batch 78: 100%|██████████| 100/100 [00:00<00:00, 681.96it/s]\nProcessing batch 79: 100%|██████████| 100/100 [00:00<00:00, 670.74it/s]\nProcessing batch 80: 100%|██████████| 100/100 [00:00<00:00, 667.37it/s]\nProcessing batch 81: 100%|██████████| 100/100 [00:00<00:00, 676.89it/s]\nProcessing batch 82: 100%|██████████| 100/100 [00:00<00:00, 668.52it/s]\nProcessing batch 83: 100%|██████████| 100/100 [00:00<00:00, 637.56it/s]\nProcessing batch 84: 100%|██████████| 100/100 [00:00<00:00, 651.11it/s]\nProcessing batch 85:   0%|          | 0/100 [00:00<?, ?it/s]","output_type":"stream"}]},{"cell_type":"code","source":"csv_path = Path('/kaggle/working/Small_df5.csv')\ncombined_df.to_csv(csv_path, index=False)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:18:48.526613Z","iopub.status.idle":"2024-08-21T17:18:48.527034Z","shell.execute_reply.started":"2024-08-21T17:18:48.526841Z","shell.execute_reply":"2024-08-21T17:18:48.526863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import h5py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\n\ndef show_random_images_from_hdf5(hdf5_path, num_images=5):\n    \"\"\"\n    Display a random sample of images from an HDF5 file.\n\n    :param hdf5_path: Path to the HDF5 file.\n    :param num_images: Number of random images to display.\n    \"\"\"\n    try:\n        with h5py.File(hdf5_path, 'r') as hdf_file:\n            # Get all image keys from the HDF5 file\n            image_keys = list(hdf_file['oversampled_images'].keys())\n            \n            if len(image_keys) < num_images:\n                raise ValueError(f\"Not enough images in the HDF5 file to display {num_images} images.\")\n            \n            # Randomly select a subset of image keys\n            selected_keys = random.sample(image_keys, num_images)\n            \n            fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n            if num_images == 1:\n                axes = [axes]\n            \n            for ax, key in zip(axes, selected_keys):\n                # Load image data from the HDF5 file\n                img_data = hdf_file['oversampled_images'][key][()]\n                ax.imshow(img_data)\n                ax.set_title(key)\n                ax.axis('off')\n            \n            plt.show()\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Example usage\nshow_random_images_from_hdf5(output_hdf5_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:18:48.528873Z","iopub.status.idle":"2024-08-21T17:18:48.529260Z","shell.execute_reply.started":"2024-08-21T17:18:48.529074Z","shell.execute_reply":"2024-08-21T17:18:48.529090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import h5py\ndef show_random_oversampled_images_from_hdf5(hdf5_path, num_images=5, suffix=\"_oversampled_1\"):\n    \"\"\"\n    Display a random sample of oversampled images from an HDF5 file.\n\n    :param hdf5_path: Path to the HDF5 file.\n    :param num_images: Number of random images to display.\n    :param suffix: Suffix used to identify oversampled images.\n    \"\"\"\n    try:\n        with h5py.File(hdf5_path, 'r') as hdf_file:\n            # Get all image keys from the HDF5 file\n            image_keys = list(hdf_file['oversampled_images'].keys())\n            \n            # Filter keys to only include those with the suffix indicating they are oversampled\n            oversampled_keys = [key for key in image_keys if suffix in key]\n            \n            if len(oversampled_keys) < num_images:\n                raise ValueError(f\"Not enough oversampled images in the HDF5 file to display {num_images} images.\")\n            \n            # Randomly select a subset of oversampled image keys\n            selected_keys = random.sample(oversampled_keys, num_images)\n            \n            fig, axes = plt.subplots(1, num_images, figsize=(15, 5))\n            if num_images == 1:\n                axes = [axes]\n            \n            for ax, key in zip(axes, selected_keys):\n                # Load image data from the HDF5 file\n                img_data = hdf_file['oversampled_images'][key][()]\n                ax.imshow(img_data)\n                ax.set_title(key)\n                ax.axis('off')\n            \n            plt.show()\n    \n    except Exception as e:\n        print(f\"An error occurred: {e}\")\noutput_hdf5_path = Path(\"/kaggle/working/Oversampled_small.hdf5\")\n# Example usage\nshow_random_oversampled_images_from_hdf5(output_hdf5_path)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:18:48.530998Z","iopub.status.idle":"2024-08-21T17:18:48.531376Z","shell.execute_reply.started":"2024-08-21T17:18:48.531199Z","shell.execute_reply":"2024-08-21T17:18:48.531214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df[combined_df['isic_id'].str.contains('_oversampled_')]\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:18:48.532987Z","iopub.status.idle":"2024-08-21T17:18:48.533342Z","shell.execute_reply.started":"2024-08-21T17:18:48.533170Z","shell.execute_reply":"2024-08-21T17:18:48.533185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df","metadata":{"execution":{"iopub.status.busy":"2024-08-21T17:18:48.535024Z","iopub.status.idle":"2024-08-21T17:18:48.535422Z","shell.execute_reply.started":"2024-08-21T17:18:48.535248Z","shell.execute_reply":"2024-08-21T17:18:48.535264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" def count_datasets(name, obj, counter):\n    \"\"\"Recursively count the datasets in the HDF5 file.\"\"\"\n    if isinstance(obj, h5py.Group):\n        for key, item in obj.items():\n            count_datasets(f\"{name}/{key}\", item, counter)\n    elif isinstance(obj, h5py.Dataset):\n        counter[0] += 1\n\ndef print_hdf5_structure_and_count(file_path):\n    \"\"\"Open an HDF5 file, print its structure, and count the total number of images (datasets).\"\"\"\n    with h5py.File(file_path, 'r') as f:\n        print(\"HDF5 file structure:\")\n        dataset_count = [0]\n        for key in f.keys():\n            count_datasets(key, f[key], dataset_count)\n        print(f\"\\nTotal number of images: {dataset_count[0]}\")\n\n# Example usage\nprint_hdf5_structure_and_count('/kaggle/working/Oversampled_small.hdf5')","metadata":{"execution":{"iopub.status.busy":"2024-08-21T18:02:32.669396Z","iopub.execute_input":"2024-08-21T18:02:32.670362Z","iopub.status.idle":"2024-08-21T18:03:09.080269Z","shell.execute_reply.started":"2024-08-21T18:02:32.670326Z","shell.execute_reply":"2024-08-21T18:03:09.079197Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"HDF5 file structure:\n\nTotal number of images: 105178\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}